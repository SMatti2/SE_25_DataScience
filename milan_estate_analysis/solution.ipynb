{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The streamlit module is not an IPython extension.\n"
     ]
    }
   ],
   "source": [
    "import filecmp\n",
    "import zipfile\n",
    "import os\n",
    "import streamlit as st\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import folium\n",
    "import geopandas as gpd\n",
    "import branca.colormap as cm\n",
    "%load_ext streamlit\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Unzip and rename the files\n",
    "Each zip_file have two files: \n",
    "- One with the names of the neighborhoods\n",
    "- One with the rent and sale values for those six months\n",
    "\n",
    "So, one year has four files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "zip_file_path = './zipped_files'\n",
    "unzipped_file_path = './unzipped_files'\n",
    "\n",
    "if not os.path.exists(unzipped_file_path):\n",
    "    for file in os.listdir(zip_file_path):\n",
    "        zip_ref = zipfile.ZipFile(os.path.join(zip_file_path, file), 'r')\n",
    "        zip_ref.extractall(unzipped_file_path)\n",
    "        zip_ref.close()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 8\u001b[0m\n\u001b[1;32m      6\u001b[0m parts \u001b[39m=\u001b[39m file\u001b[39m.\u001b[39msplit(\u001b[39m'\u001b[39m\u001b[39m_\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m      7\u001b[0m new_parts \u001b[39m=\u001b[39m parts[\u001b[39m3\u001b[39m:]\n\u001b[0;32m----> 8\u001b[0m new_parts[\u001b[39m0\u001b[39m] \u001b[39m=\u001b[39m new_parts[\u001b[39m0\u001b[39;49m][:\u001b[39m4\u001b[39m] \u001b[39m+\u001b[39m \u001b[39m'\u001b[39m\u001b[39m_\u001b[39m\u001b[39m'\u001b[39m \u001b[39m+\u001b[39m new_parts[\u001b[39m0\u001b[39m][\u001b[39m4\u001b[39m:]\n\u001b[1;32m      9\u001b[0m new_file \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39m_\u001b[39m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mjoin(new_parts)\n\u001b[1;32m     11\u001b[0m os\u001b[39m.\u001b[39mrename(os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mjoin(unzipped_file_path, file),\n\u001b[1;32m     12\u001b[0m           os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mjoin(unzipped_file_path, new_file))\n",
      "\u001b[0;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "# Check if a the first file in the unzipped folder is with has number in the first letter of the file name\n",
    "\n",
    "if not os.listdir(unzipped_file_path)[0][0].isdigit():\n",
    "    for file in os.listdir(unzipped_file_path):\n",
    "        if file.endswith('.csv'):\n",
    "            parts = file.split('_')\n",
    "            new_parts = parts[3:]\n",
    "            new_parts[0] = new_parts[0][:4] + '_' + new_parts[0][4:]\n",
    "            new_file = '_'.join(new_parts)\n",
    "\n",
    "            os.rename(os.path.join(unzipped_file_path, file),\n",
    "                      os.path.join(unzipped_file_path, new_file))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clean the structures and the file names\n",
    "\n",
    "First of all we need to check if the neighborhoods files are always the same, and if so, we'll just consider one and call it NEIGHBORHOODS.csv and rename the rest with VALUES_year_1.csv or VALUES_year_2.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The files are not all the same\n"
     ]
    }
   ],
   "source": [
    "if len(os.listdir(unzipped_file_path)) > 22:\n",
    "    zone_files = [f for f in os.listdir(unzipped_file_path) if 'ZONE' in f]\n",
    "    zone_files.sort()\n",
    "\n",
    "    # Check if the Zone files are the same\n",
    "    first_file = os.path.join(unzipped_file_path, zone_files[0])\n",
    "    \n",
    "    ## Exclude the first line of the file\n",
    "    with open(first_file, 'r') as f:\n",
    "        first_file_lines = f.readlines()[1:]\n",
    "    \n",
    "    for file in zone_files[1:]:\n",
    "        other_zone_file = os.path.join(unzipped_file_path, file)\n",
    "        \n",
    "        ## Exclude the first line of the file\n",
    "        with open(other_zone_file, 'r') as f:\n",
    "            other_file_lines = f.readlines()[1:]\n",
    "\n",
    "        are_equal = first_file_lines == other_file_lines\n",
    "    \n",
    "        if not are_equal:\n",
    "            print(f'The files are not all the same')\n",
    "            break\n",
    "    if are_equal:\n",
    "        print('All the files are the same')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unfortunately the files with the neighbors names are **not** all the same. Let's check if they also change between semesters of the same year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All the files are the same\n"
     ]
    }
   ],
   "source": [
    "if len(os.listdir(unzipped_file_path)) > 22:\n",
    "    checker = 1\n",
    "    # Compare the 'zone' files of the same year\n",
    "    for index, file in enumerate(zone_files):\n",
    "        if checker != 0:\n",
    "            other_zone_file = os.path.join(\n",
    "                unzipped_file_path, zone_files[index + 1])\n",
    "\n",
    "            # Exclude the first line of the file\n",
    "            with open(os.path.join(unzipped_file_path, file), 'r') as f1, open(os.path.join(unzipped_file_path, zone_files[index + 1]), 'r') as f2:\n",
    "                file1_lines = f1.readlines()[1:]\n",
    "                file2_lines = f2.readlines()[1:]\n",
    "\n",
    "            are_equal = file1_lines == file2_lines\n",
    "\n",
    "            if not are_equal:\n",
    "                print(\n",
    "                    f'\\n The files are not all the same')\n",
    "                break\n",
    "            checker = 0\n",
    "        else:\n",
    "            continue\n",
    "\n",
    "    if are_equal:\n",
    "        print('All the files are the same')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The neighborhoods file stay the same between semester, so we will remove one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for file_name in zone_files:\n",
    "    if file_name.endswith('_2_ZONE.csv'):\n",
    "        file = os.path.join(unzipped_file_path, file_name)\n",
    "        os.remove(file)\n",
    "\n",
    "for file_name in zone_files:\n",
    "    if file_name.endswith('_1_ZONE.csv'):\n",
    "        # Rename the file keeping the year\n",
    "        year = file_name[:4]\n",
    "\n",
    "        os.rename(os.path.join(unzipped_file_path, file_name),\n",
    "                    os.path.join(unzipped_file_path, f'{year}_ZONE.csv'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Open all the Rent/Sale files in DataFrames and merge them together in one\n",
    "**We will keep only the neighborhoods present in all of them**\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a function that given the OMI files returns one dataframe\n",
    "def create_dataframe_from_OMI_files(neighborhoods_file, values_file1, values_file2):\n",
    "    neighborhoods = pd.read_csv(neighborhoods_file, sep=';', skiprows=1)\n",
    "    values1 = pd.read_csv(values_file1, sep=';', skiprows=1)\n",
    "    values2 = pd.read_csv(values_file2, sep=';', skiprows=1)\n",
    "    year = values_file1.split('/')[2].split('_')[0]\n",
    "\n",
    "    # Drop columns that are not needed\n",
    "    neighborhoods.drop(columns=['Area_territoriale', 'Regione', 'Prov', 'Comune_ISTAT', 'Comune_cat', 'Sez', 'Comune_amm',\n",
    "                                'Comune_descrizione', 'Fascia', 'LinkZona', 'Cod_tip_prev', 'Stato_prev', 'Microzona'], inplace=True, errors='ignore')\n",
    "    values1.drop(columns=['Area_territoriale', 'Regione', 'Prov', 'Comune_ISTAT', 'Comune_cat', 'Sez', 'Comune_amm',\n",
    "                          'Comune_descrizione', 'Fascia', 'LinkZona', 'Cod_Tip', 'Stato_prev', 'Sup_NL_compr', 'Sup_NL_loc'], inplace=True, errors='ignore')\n",
    "    values2.drop(columns=['Area_territoriale', 'Regione', 'Prov', 'Comune_ISTAT', 'Comune_cat', 'Sez', 'Comune_amm',\n",
    "                          'Comune_descrizione', 'Fascia', 'LinkZona', 'Cod_Tip', 'Stato_prev', 'Sup_NL_compr', 'Sup_NL_loc'], inplace=True, errors='ignore')\n",
    "\n",
    "    # Add columns to the values files regarding the semester\n",
    "    values1['Semestre'] = 1\n",
    "    values2['Semestre'] = 2\n",
    "    \n",
    "    values = pd.concat([values1, values2])\n",
    "\n",
    "    neighborhoods_values = pd.merge(neighborhoods, values, on='Zona')\n",
    "    neighborhoods_values.drop(columns=['Zona'], inplace=True)\n",
    "    neighborhoods_values['Anno'] = year\n",
    "\n",
    "    # Drop unnamed columns\n",
    "    neighborhoods_values.drop(columns=[\n",
    "                              col for col in neighborhoods_values.columns if 'Unnamed' in col], inplace=True)\n",
    "\n",
    "    neighborhoods_values.drop(columns=['Area_territoriale', 'Regione', 'Prov', 'Comune_ISTAT', 'Comune_cat', 'Sez', 'Comune_amm',\n",
    "                                       'Comune_descrizione', 'Fascia', 'LinkZona', 'Cod_tip_prev', 'Stato_prev', 'Microzona',\n",
    "                                       'Area_territoriale', 'Regione', 'Prov', 'Comune_ISTAT', 'Comune_cat', 'Sez', 'Comune_amm',\n",
    "                                       'Comune_descrizione', 'Fascia', 'LinkZona', 'Cod_Tip', 'Stato_prev', 'Sup_NL_compr', 'Sup_NL_loc'], inplace=True, errors='ignore')\n",
    "\n",
    "    return neighborhoods_values\n",
    "\n",
    "\n",
    "# create_dataframe_from_OMI_files(os.path.join(unzipped_file_path, '2013_ZONE.csv'), os.path.join(unzipped_file_path, '2013_1_VALORI.csv'), os.path.join(unzipped_file_path, '2013_2_VALORI.csv'))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "checker = 0\n",
    "csvs_separated_by_year_path = './merged_csvs'\n",
    "os.mkdir(csvs_separated_by_year_path) if not os.path.exists(\n",
    "    csvs_separated_by_year_path) else None\n",
    "\n",
    "\n",
    "if len(os.listdir(csvs_separated_by_year_path)) < 1:\n",
    "    # Sort the files by year\n",
    "    csv_files = sorted([f for f in os.listdir(\n",
    "        unzipped_file_path) if f.endswith('.csv')])\n",
    "\n",
    "    for index, file in enumerate(csv_files):\n",
    "        year = file.split('_')[0]\n",
    "\n",
    "        checker = 1 if year not in csv_files[index + 2] else 0\n",
    "\n",
    "        if checker == 1:\n",
    "            continue\n",
    "        else:\n",
    "            values_file1 = os.path.join(\n",
    "                unzipped_file_path, file)\n",
    "\n",
    "            values_file2 = os.path.join(\n",
    "                unzipped_file_path, csv_files[index + 1])\n",
    "\n",
    "            neighborhoods_file = os.path.join(\n",
    "                unzipped_file_path, csv_files[index + 2])\n",
    "\n",
    "            df = create_dataframe_from_OMI_files(\n",
    "                neighborhoods_file, values_file1, values_file2)\n",
    "            df.reset_index(drop=True, inplace=True)\n",
    "            df.to_csv(\n",
    "                os.path.join(os.getcwd(), csvs_separated_by_year_path, year + '_values.csv'), index=False, columns=[\n",
    "                    'Zona_Descr', 'Descr_Tipologia', 'Stato', 'Compr_min', 'Compr_max', 'Loc_min', 'Loc_max', 'Semestre', 'Anno'])\n",
    "\n",
    "        if year == '2022':\n",
    "            break\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before merging all of the different years in one file, we will have to make a row for each neighborhood, because at the moment neighborhood with the same values are displayed on the same line and are grouped differently in each dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for file in os.listdir(csvs_separated_by_year_path):\n",
    "    df = pd.read_csv(os.path.join(csvs_separated_by_year_path, file))\n",
    "\n",
    "    df['Zona_Descr'] = df['Zona_Descr'].str.strip(\"-'\")\n",
    "    # Split the Zona_Descr column by comma and explode it to create a new row for each zone\n",
    "    df = df.assign(Zona_Descr=df['Zona_Descr'].str.split(\", \")).explode('Zona_Descr')\n",
    "    df = df.reset_index(drop=True)\n",
    "\n",
    "    # Save the file as a csv overwriting the old one\n",
    "    df.to_csv(os.path.join(csvs_separated_by_year_path, file), index=False)\n",
    "\n",
    "\n",
    "# Create a function that given the OMI files returns one dataframe\n",
    "def concat_csvs(csv_files):\n",
    "    dfs = []\n",
    "    for file in csv_files:\n",
    "        df = pd.read_csv(os.path.join(csvs_separated_by_year_path, file))\n",
    "        dfs.append(df)\n",
    "    concatenated_df = pd.concat(dfs, ignore_index=True)\n",
    "    return concatenated_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Merge all the csv files of the neighborhoods sale and rent values in one DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nei_sale_rent = concat_csvs(os.listdir(csvs_separated_by_year_path))\n",
    "nei_sale_rent.rename(columns={'Zona_Descr': 'Neighborhood', 'Descr_Tipologia': 'Type', 'Stato': 'Status', 'Compr_min': 'Min_Sale_Price',\n",
    "                     'Compr_max': 'Max_Sale_Price', 'Loc_min': 'Min_Rent_Price', 'Loc_max': 'Max_Rent_Price', 'Semestre': 'Semester', 'Anno': 'Year'}, inplace=True)\n",
    "\n",
    "# Change the types of the columns in float and int\n",
    "nei_sale_rent['Min_Rent_Price'] = nei_sale_rent['Min_Rent_Price'].astype(str).str.replace(',', '.').astype(float)\n",
    "nei_sale_rent['Max_Rent_Price'] = nei_sale_rent['Max_Rent_Price'].astype(str).str.replace(',', '.').astype(float)\n",
    "nei_sale_rent['Min_Sale_Price'] = nei_sale_rent['Min_Sale_Price'].astype(int)\n",
    "nei_sale_rent['Max_Sale_Price'] = nei_sale_rent['Max_Sale_Price'].astype(int)\n",
    "nei_sale_rent['Semester'] = nei_sale_rent['Semester'].astype(int)\n",
    "nei_sale_rent['Year'] = nei_sale_rent['Year'].astype(int)\n",
    "\n",
    "# Make neighborhoods without a comma\n",
    "nei_sale_rent['Neighborhood'] = nei_sale_rent['Neighborhood'].str.replace(',', '')\n",
    "# Check if neighborhoods have - and take the second part of the string\n",
    "nei_sale_rent['Neighborhood'] = nei_sale_rent['Neighborhood'].apply(lambda x: x.split('-')[1] if '-' in x else x)\n",
    "# Make CNA MERLATA A and C.NA MERLATA neighborhoods the same in CAMERLATA\n",
    "nei_sale_rent['Neighborhood'] = nei_sale_rent['Neighborhood'].apply(lambda x: 'CAMERLATA' if 'C.NA MERLATA' in x else x)\n",
    "nei_sale_rent['Neighborhood'] = nei_sale_rent['Neighborhood'].apply(lambda x: 'CAMERLATA' if 'CNA MERLATA' in x else x)\n",
    "nei_sale_rent['Neighborhood'] = nei_sale_rent['Neighborhood'].apply(lambda x: 'SAN CARLO' if 'SAN CARLO B.' in x else x)\n",
    "# Remove spaces from neighborhoods\n",
    "nei_sale_rent['Neighborhood'] = nei_sale_rent['Neighborhood'].str.strip()\n",
    "# Add the average sale and rent price \n",
    "nei_sale_rent['Avg_Sale_Price'] = (\n",
    "    nei_sale_rent['Min_Sale_Price'] + nei_sale_rent['Max_Sale_Price']) / 2\n",
    "nei_sale_rent['Avg_Rent_Price'] = (\n",
    "    nei_sale_rent['Min_Rent_Price'] + nei_sale_rent['Max_Rent_Price']) / 2\n",
    "\n",
    "nei_sale_rent.to_csv(os.path.join(os.getcwd(), 'rent_sale_per_neighborhood.csv'), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import difflib\n",
    "\n",
    "\n",
    "def modify_array(array1, array2):\n",
    "    modified_array2 = []\n",
    "    \n",
    "    for item2 in array2:\n",
    "        closest_match = difflib.get_close_matches(item2, array1, n=1, cutoff=0.3)\n",
    "        if closest_match:\n",
    "            modified_array2.append(closest_match[0])\n",
    "        else:\n",
    "            modified_array2.append(item2)\n",
    "    \n",
    "    return modified_array2"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modify the geojson file "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Neighborhoods in geojson but not in OMI files:  {'QUARTO CAGNINO', 'DERGANO', 'TALIEDO - MORSENCHIO - Q.RE FORLANINI', 'SAN SIRO', 'ISOLA', 'QT 8', 'GUASTALLA', 'MAGGIORE - MUSOCCO - CERTOSA', 'DE ANGELI - MONTE ROSA', 'TRE TORRI', 'GRATOSOGLIO - Q.RE MISSAGLIA - Q.RE TERRAZZE', 'PORTA VIGENTINA - PORTA LODOVICA', \"MONLUE' - PONTE LAMBRO\", 'TIBALDI', 'STADERA - CHIESA ROSSA - Q.RE TORRETTA - CONCA FALLATA', 'BAGGIO - Q.RE DEGLI OLMI - Q.RE VALSESIA', \"PARCO BOSCO IN CITTA'\", 'FORZE ARMATE', 'CASCINA MERLATA', 'PTA ROMANA', 'BANDE NERE', 'ASSIANO', 'SCALO ROMANA', 'STADIO - IPPODROMI', 'CIMIANO - ROTTOLE - Q.RE FELTRE', 'BRUZZANO', 'GRECO - SEGNANO', 'MONCUCCO - SAN CRISTOFORO', 'STAZIONE CENTRALE - PONTE SEVESO', 'PADOVA - TURRO - CRESCENZAGO', 'TRENNO', 'PARCO DEI NAVIGLI', 'PORTA GARIBALDI - PORTA NUOVA', 'MAGENTA - S. VITTORE', 'PORTA MAGENTA', 'GHISOLFA', 'Q.RE GALLARATESE - Q.RE SAN LEONARDO - LAMPUGNANO', 'LAMBRATE - ORTICA', 'LODI - CORVETTO', 'PARCO DELLE ABBAZIE', 'BUENOS AIRES - PORTA VENEZIA - PORTA MONFORTE', 'GORLA - PRECOTTO', 'PORTELLO', 'VILLAPIZZONE - CAGNOLA - BOLDINASCO', 'RONCHETTO DELLE RANE', 'MACIACHINI - MAGGIOLINA', 'QUINTO ROMANO', 'QUARTO OGGIARO - VIALBA - MUSOCCO', 'LORETO - CASORETTO - NOLO', 'QUINTOSOLE', 'STEPHENSON', 'VIGENTINO - Q.RE FATIMA', 'GIARDINI P.TA VENEZIA', 'PORTA TICINESE - CONCA DEL NAVIGLIO', 'MORIVIONE', 'ADRIANO', 'RONCHETTO SUL NAVIGLIO - Q.RE LODOVICO IL MORO', 'PARCO FORLANINI - CAVRIANO', \"NIGUARDA - CA' GRANDA - PRATO CENTENARO - Q.RE FULVIO TESTI\", 'TRIULZO SUPERIORE', 'CANTALUPA', 'UMBRIA - MOLISE - CALVAIRATE', 'ROGOREDO - SANTA GIULIA', \"CITTA' STUDI\", 'PORTA TICINESE - CONCHETTA'}\n",
      "['LUDOVICO IL MORO' 'P. TRENNO' 'CASTELLO' 'BOVISASCA' 'PARCO NORD'\n",
      " 'FIGINO' 'ORTOMERCATO' 'QUARTO OGGIARO' 'PIOLA' 'Q. CAGNINO' 'IPPODROMO'\n",
      " 'Q. ROMANO' 'DUOMO' 'SAN CARLO' 'COMASINA' 'ITALIA' 'TIRO SEGNO'\n",
      " 'MONTE ROSA' 'FARINI' 'ABRUZZI' 'SEMPIONE' 'CAGNOLA' 'UDINE'\n",
      " 'PARCO SEMPIONE' 'BARONA' 'STAZIONE CENTRALE VIALE STELVIO'\n",
      " 'PARCO TROTTER' 'PRECOTTO' 'FULVIO TESTI' 'ROSERIO' 'PORTA ROMANA'\n",
      " 'FORLANINI' 'PORTA TICINESE' 'TURATI' 'PAGANO' 'VIGENTINO' 'BICOCCA'\n",
      " 'LORETO' 'MUGGIANO' 'VIA SAN VITTORE' 'RONCHETTO' 'GALLARATESE'\n",
      " 'PARCO LAMBRO' 'CRESCENZAGO' 'GRATOSOGLIO' 'CERMENTATE' 'CHIARAVALLE'\n",
      " 'ARCO DELLA PACE' 'MAGGIOLINA' 'WAGNER' 'LAMBRATE' 'CHIESA ROSSA'\n",
      " 'PORTA VIGENTINA' 'XXII MARZO' 'NAVIGLIO GRANDE' 'INDIPENDENZA'\n",
      " 'PORTA NUOVA' 'AFFORI' 'BUENOS AIRES' 'SAN CRISTOFORO' 'BOVISA'\n",
      " 'GIULIO CESARE' 'GIAMBELLINO' 'CITY LIFE' 'PORTA GENOVA' 'CORSICA'\n",
      " 'CAMERLATA' 'SARPI' 'BRERA' 'SANTA GIULIA' 'TORINO' 'LORENTEGGIO']\n"
     ]
    }
   ],
   "source": [
    "# Import geojson\n",
    "neighborhood = gpd.GeoDataFrame.from_file(\n",
    "    './milan_districts.geojson')\n",
    "\n",
    "# Check which neighborhoods are in the geojson file but not in the OMI files\n",
    "neighborhoods_in_geojson = neighborhood['NIL'].unique()\n",
    "neighborhoods_in_omi = nei_sale_rent['Neighborhood'].unique()\n",
    "\n",
    "print('Neighborhoods in geojson but not in OMI files: ', set(\n",
    "    neighborhoods_in_geojson) - set(neighborhoods_in_omi))\n",
    "\n",
    "# Modify NIL column in the geojson file with the function modify_array\n",
    "neighborhood['NIL'] = modify_array(neighborhoods_in_omi, neighborhoods_in_geojson)\n",
    "print(neighborhood['NIL'].unique())\n",
    "\n",
    "# Save the new geojson file\n",
    "neighborhood.to_file('./milan_districts_modified.geojson', driver='GeoJSON')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fix Mispellings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['id', 'listing_url', 'scrape_id', 'last_scraped', 'source', 'name',\n",
       "       'description', 'neighborhood_overview', 'picture_url', 'host_id',\n",
       "       'host_url', 'host_name', 'host_since', 'host_location', 'host_about',\n",
       "       'host_response_time', 'host_response_rate', 'host_acceptance_rate',\n",
       "       'host_is_superhost', 'host_thumbnail_url', 'host_picture_url',\n",
       "       'host_neighbourhood', 'host_listings_count',\n",
       "       'host_total_listings_count', 'host_verifications',\n",
       "       'host_has_profile_pic', 'host_identity_verified', 'neighborhood',\n",
       "       'neighborhood_cleansed', 'neighborhood_group_cleansed', 'latitude',\n",
       "       'longitude', 'property_type', 'room_type', 'accommodates', 'bathrooms',\n",
       "       'bathrooms_text', 'bedrooms', 'beds', 'amenities', 'price',\n",
       "       'minimum_nights', 'maximum_nights', 'minimum_minimum_nights',\n",
       "       'maximum_minimum_nights', 'minimum_maximum_nights',\n",
       "       'maximum_maximum_nights', 'minimum_nights_avg_ntm',\n",
       "       'maximum_nights_avg_ntm', 'calendar_updated', 'has_availability',\n",
       "       'availability_30', 'availability_60', 'availability_90',\n",
       "       'availability_365', 'calendar_last_scraped', 'number_of_reviews',\n",
       "       'number_of_reviews_ltm', 'number_of_reviews_l30d', 'first_review',\n",
       "       'last_review', 'review_scores_rating', 'review_scores_accuracy',\n",
       "       'review_scores_cleanliness', 'review_scores_checkin',\n",
       "       'review_scores_communication', 'review_scores_location',\n",
       "       'review_scores_value', 'license', 'instant_bookable',\n",
       "       'calculated_host_listings_count',\n",
       "       'calculated_host_listings_count_entire_homes',\n",
       "       'calculated_host_listings_count_private_rooms',\n",
       "       'calculated_host_listings_count_shared_rooms', 'reviews_per_month'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "airbnb = pd.read_csv('./airbnb_listings2022.csv')\n",
    "\n",
    "# Fix mispelled column names\n",
    "corrections = {\n",
    "    'neighbourhood': 'neighborhood',\n",
    "    'neighbourhood_cleansed': 'neighborhood_cleansed',\n",
    "    'neighbourhood_group_cleansed': 'neighborhood_group_cleansed',\n",
    "}\n",
    "\n",
    "airbnb.rename(columns=corrections, inplace=True)\n",
    "airbnb.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([358, 363,  88, 173,   0, 184, 219, 327, 203,  78, 359,  98,  52,\n",
       "       163,  55, 355, 177, 239, 204,  67, 112, 364, 297,  21,  14, 362,\n",
       "       268, 246, 349, 350, 322, 365, 346, 142, 192, 299,   1, 138, 345,\n",
       "       151,  68,  90, 225, 348,  36, 179, 123,  81, 220, 174, 205, 157,\n",
       "       166, 330,  95,  20, 224, 356,   9, 263,  79,  82,  28, 242, 172,\n",
       "       334,  18, 354, 258, 303, 296, 264, 212, 295,  87, 245,  37,  76,\n",
       "       180, 160,  97, 120, 159,  13, 276,  26, 227, 357, 352, 314, 312,\n",
       "       342,  50, 269, 234, 206, 215, 213, 185,  42,   6,  94, 110, 241,\n",
       "       320, 315,  93, 272, 158, 101, 323, 178, 265, 351,  89,  19, 298,\n",
       "       240, 347,  84, 317, 169,  11,  47, 235, 168, 164, 201,  32, 236,\n",
       "        35,  71, 293, 286, 171,   8, 167, 217,  85, 232,  59, 301,  83,\n",
       "        54, 156, 343, 332,  99,  65, 183,  45,  12, 360, 339, 146, 108,\n",
       "        64, 319, 161, 337,  40, 200, 153, 150,  66, 300, 111, 139, 221,\n",
       "       321, 251, 214,   4, 261, 207, 291, 230, 211, 182,   2, 244, 145,\n",
       "       190, 255, 325,  34, 305, 154, 309, 259, 294,  22,  80, 308, 335,\n",
       "        10, 338, 290,  62, 247, 336, 254, 176, 194, 175,  60,   3,  51,\n",
       "       324, 114, 100,  23,  86, 243, 195, 237, 152, 249, 116, 340, 257,\n",
       "       273, 189, 129, 281,  48, 250,  29,  70,  53, 104, 285, 307, 188,\n",
       "       344,  49, 199, 228, 102, 233, 223,  17, 127,   5,  46, 361, 210,\n",
       "        30, 341,  63, 333, 155, 128, 137, 292,  92, 113, 143,  44, 316,\n",
       "       328, 302, 226, 165, 140,  69, 252,  56,  15, 274, 181, 304, 310,\n",
       "       198, 119, 118,  33, 222, 141, 115,  72, 280, 193, 282, 107, 248,\n",
       "       121, 144, 187, 216, 331, 271, 135, 218,  77, 238, 266, 262, 209,\n",
       "       191,  31,  38, 329, 267,  27, 162, 136,  16, 277, 279, 149,  41,\n",
       "       208,  96, 122, 284,  61,  74, 170,  73, 326,  24, 253, 270, 147,\n",
       "        43,  57, 287,  75, 229, 186, 275, 131,  25, 105, 318, 289, 125,\n",
       "       283,   7, 311, 260, 313, 109, 126, 306, 134, 148, 231,  58, 288,\n",
       "        91, 278, 256, 196, 353,  39, 197, 133, 117, 103, 202, 130, 132,\n",
       "       124, 106])"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "airbnb['availability_365'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select only the columns that are useful for the analysis\n",
    "bnb = airbnb[['id', 'name', 'neighborhood_cleansed', 'latitude', 'longitude', 'host_listings_count', 'property_type', 'room_type', 'accommodates', 'bedrooms', 'price', 'availability_365', 'review_scores_rating']]\n",
    "# Save the file\n",
    "bnb.to_csv('./airbnb_listings2022_modified.csv', index=False)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
