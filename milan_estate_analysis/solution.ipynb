{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The streamlit module is not an IPython extension.\n"
     ]
    }
   ],
   "source": [
    "import filecmp\n",
    "import zipfile\n",
    "import os\n",
    "import streamlit as st\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import folium\n",
    "import geopandas as gpd\n",
    "import branca.colormap as cm\n",
    "%load_ext streamlit\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Unzip and rename the files\n",
    "Each zip_file have two files: \n",
    "- One with the names of the neighborhoods\n",
    "- One with the rent and sale values for those six months\n",
    "\n",
    "So, one year has four files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "zip_file_path = './zipped_files'\n",
    "unzipped_file_path = './unzipped_files'\n",
    "\n",
    "if not os.path.exists(unzipped_file_path):\n",
    "    for file in os.listdir(zip_file_path):\n",
    "        zip_ref = zipfile.ZipFile(os.path.join(zip_file_path, file), 'r')\n",
    "        zip_ref.extractall(unzipped_file_path)\n",
    "        zip_ref.close()\n",
    "    for file in os.listdir(unzipped_file_path):\n",
    "        if file.endswith('.csv'):\n",
    "            parts = file.split('_')\n",
    "            new_parts = parts[3:]\n",
    "            new_parts[0] = new_parts[0][:4] + '_' + new_parts[0][4:]\n",
    "            new_file = '_'.join(new_parts)\n",
    "\n",
    "            os.rename(os.path.join(unzipped_file_path, file),\n",
    "                      os.path.join(unzipped_file_path, new_file))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clean the structures and the file names\n",
    "\n",
    "First of all we need to check if the neighborhoods files are always the same, and if so, we'll just consider one and call it NEIGHBORHOODS.csv and rename the rest with VALUES_year_1.csv or VALUES_year_2.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The files are not all the same\n"
     ]
    }
   ],
   "source": [
    "if len(os.listdir(unzipped_file_path)) > 22:\n",
    "    zone_files = [f for f in os.listdir(unzipped_file_path) if 'ZONE' in f]\n",
    "    zone_files.sort()\n",
    "\n",
    "    # Check if the Zone files are the same\n",
    "    first_file = os.path.join(unzipped_file_path, zone_files[0])\n",
    "    \n",
    "    ## Exclude the first line of the file\n",
    "    with open(first_file, 'r') as f:\n",
    "        first_file_lines = f.readlines()[1:]\n",
    "    \n",
    "    for file in zone_files[1:]:\n",
    "        other_zone_file = os.path.join(unzipped_file_path, file)\n",
    "        \n",
    "        ## Exclude the first line of the file\n",
    "        with open(other_zone_file, 'r') as f:\n",
    "            other_file_lines = f.readlines()[1:]\n",
    "\n",
    "        are_equal = first_file_lines == other_file_lines\n",
    "    \n",
    "        if not are_equal:\n",
    "            print(f'The files are not all the same')\n",
    "            break\n",
    "    if are_equal:\n",
    "        print('All the files are the same')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unfortunately the files with the neighbors names are **not** all the same. Let's check if they also change between semesters of the same year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All the files are the same\n"
     ]
    }
   ],
   "source": [
    "if len(os.listdir(unzipped_file_path)) > 22:\n",
    "    checker = 1\n",
    "    # Compare the 'zone' files of the same year\n",
    "    for index, file in enumerate(zone_files):\n",
    "        if checker != 0:\n",
    "            other_zone_file = os.path.join(\n",
    "                unzipped_file_path, zone_files[index + 1])\n",
    "\n",
    "            # Exclude the first line of the file\n",
    "            with open(os.path.join(unzipped_file_path, file), 'r') as f1, open(os.path.join(unzipped_file_path, zone_files[index + 1]), 'r') as f2:\n",
    "                file1_lines = f1.readlines()[1:]\n",
    "                file2_lines = f2.readlines()[1:]\n",
    "\n",
    "            are_equal = file1_lines == file2_lines\n",
    "\n",
    "            if not are_equal:\n",
    "                print(\n",
    "                    f'\\n The files are not all the same')\n",
    "                break\n",
    "            checker = 0\n",
    "        else:\n",
    "            continue\n",
    "\n",
    "    if are_equal:\n",
    "        print('All the files are the same')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The neighborhoods file stay the same between semester, so we will remove one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "for file_name in zone_files:\n",
    "    if file_name.endswith('_2_ZONE.csv'):\n",
    "        file = os.path.join(unzipped_file_path, file_name)\n",
    "        os.remove(file)\n",
    "\n",
    "for file_name in zone_files:\n",
    "    if file_name.endswith('_1_ZONE.csv'):\n",
    "        # Rename the file keeping the year\n",
    "        year = file_name[:4]\n",
    "\n",
    "        os.rename(os.path.join(unzipped_file_path, file_name),\n",
    "                    os.path.join(unzipped_file_path, f'{year}_ZONE.csv'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Open all the Rent/Sale files in DataFrames and merge them together in one\n",
    "**We will keep only the neighborhoods present in all of them**\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a function that given the OMI files returns one dataframe\n",
    "def create_dataframe_from_OMI_files(neighborhoods_file, values_file1, values_file2):\n",
    "    neighborhoods = pd.read_csv(neighborhoods_file, sep=';', skiprows=1)\n",
    "    values1 = pd.read_csv(values_file1, sep=';', skiprows=1)\n",
    "    values2 = pd.read_csv(values_file2, sep=';', skiprows=1)\n",
    "    year = values_file1.split('/')[2].split('_')[0]\n",
    "\n",
    "    # Drop columns that are not needed\n",
    "    neighborhoods.drop(columns=['Area_territoriale', 'Regione', 'Prov', 'Comune_ISTAT', 'Comune_cat', 'Sez', 'Comune_amm',\n",
    "                                'Comune_descrizione', 'Fascia', 'LinkZona', 'Cod_tip_prev', 'Stato_prev', 'Microzona'], inplace=True, errors='ignore')\n",
    "    values1.drop(columns=['Area_territoriale', 'Regione', 'Prov', 'Comune_ISTAT', 'Comune_cat', 'Sez', 'Comune_amm',\n",
    "                          'Comune_descrizione', 'Fascia', 'LinkZona', 'Cod_Tip', 'Stato_prev', 'Sup_NL_compr', 'Sup_NL_loc'], inplace=True, errors='ignore')\n",
    "    values2.drop(columns=['Area_territoriale', 'Regione', 'Prov', 'Comune_ISTAT', 'Comune_cat', 'Sez', 'Comune_amm',\n",
    "                          'Comune_descrizione', 'Fascia', 'LinkZona', 'Cod_Tip', 'Stato_prev', 'Sup_NL_compr', 'Sup_NL_loc'], inplace=True, errors='ignore')\n",
    "\n",
    "    # Add columns to the values files regarding the semester\n",
    "    values1['Semestre'] = 1\n",
    "    values2['Semestre'] = 2\n",
    "    \n",
    "    values = pd.concat([values1, values2])\n",
    "\n",
    "    neighborhoods_values = pd.merge(neighborhoods, values, on='Zona')\n",
    "    neighborhoods_values.drop(columns=['Zona'], inplace=True)\n",
    "    neighborhoods_values['Anno'] = year\n",
    "\n",
    "    # Drop unnamed columns\n",
    "    neighborhoods_values.drop(columns=[\n",
    "                              col for col in neighborhoods_values.columns if 'Unnamed' in col], inplace=True)\n",
    "\n",
    "    neighborhoods_values.drop(columns=['Area_territoriale', 'Regione', 'Prov', 'Comune_ISTAT', 'Comune_cat', 'Sez', 'Comune_amm',\n",
    "                                       'Comune_descrizione', 'Fascia', 'LinkZona', 'Cod_tip_prev', 'Stato_prev', 'Microzona',\n",
    "                                       'Area_territoriale', 'Regione', 'Prov', 'Comune_ISTAT', 'Comune_cat', 'Sez', 'Comune_amm',\n",
    "                                       'Comune_descrizione', 'Fascia', 'LinkZona', 'Cod_Tip', 'Stato_prev', 'Sup_NL_compr', 'Sup_NL_loc'], inplace=True, errors='ignore')\n",
    "\n",
    "    return neighborhoods_values\n",
    "\n",
    "\n",
    "# create_dataframe_from_OMI_files(os.path.join(unzipped_file_path, '2013_ZONE.csv'), os.path.join(unzipped_file_path, '2013_1_VALORI.csv'), os.path.join(unzipped_file_path, '2013_2_VALORI.csv'))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "checker = 0\n",
    "csvs_separated_by_year_path = './merged_csvs'\n",
    "os.mkdir(csvs_separated_by_year_path) if not os.path.exists(\n",
    "    csvs_separated_by_year_path) else None\n",
    "\n",
    "\n",
    "if len(os.listdir(csvs_separated_by_year_path)) < 1:\n",
    "    # Sort the files by year\n",
    "    csv_files = sorted([f for f in os.listdir(\n",
    "        unzipped_file_path) if f.endswith('.csv')])\n",
    "\n",
    "    for index, file in enumerate(csv_files):\n",
    "        year = file.split('_')[0]\n",
    "\n",
    "        checker = 1 if year not in csv_files[index + 2] else 0\n",
    "\n",
    "        if checker == 1:\n",
    "            continue\n",
    "        else:\n",
    "            values_file1 = os.path.join(\n",
    "                unzipped_file_path, file)\n",
    "\n",
    "            values_file2 = os.path.join(\n",
    "                unzipped_file_path, csv_files[index + 1])\n",
    "\n",
    "            neighborhoods_file = os.path.join(\n",
    "                unzipped_file_path, csv_files[index + 2])\n",
    "\n",
    "            df = create_dataframe_from_OMI_files(\n",
    "                neighborhoods_file, values_file1, values_file2)\n",
    "            df.reset_index(drop=True, inplace=True)\n",
    "            df.to_csv(\n",
    "                os.path.join(os.getcwd(), csvs_separated_by_year_path, year + '_values.csv'), index=False, columns=[\n",
    "                    'Zona_Descr', 'Descr_Tipologia', 'Stato', 'Compr_min', 'Compr_max', 'Loc_min', 'Loc_max', 'Semestre', 'Anno'])\n",
    "\n",
    "        if year == '2022':\n",
    "            break\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before merging all of the different years in one file, we will have to make a row for each neighborhood, because at the moment neighborhood with the same values are displayed on the same line and are grouped differently in each dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for file in os.listdir(csvs_separated_by_year_path):\n",
    "    df = pd.read_csv(os.path.join(csvs_separated_by_year_path, file))\n",
    "\n",
    "    df['Zona_Descr'] = df['Zona_Descr'].str.strip(\"-'\")\n",
    "    # Split the Zona_Descr column by comma and explode it to create a new row for each zone\n",
    "    df = df.assign(Zona_Descr=df['Zona_Descr'].str.split(\", \")).explode('Zona_Descr')\n",
    "    df = df.reset_index(drop=True)\n",
    "\n",
    "    # Save the file as a csv overwriting the old one\n",
    "    df.to_csv(os.path.join(csvs_separated_by_year_path, file), index=False)\n",
    "\n",
    "\n",
    "# Create a function that given the OMI files returns one dataframe\n",
    "def concat_csvs(csv_files):\n",
    "    dfs = []\n",
    "    for file in csv_files:\n",
    "        df = pd.read_csv(os.path.join(csvs_separated_by_year_path, file))\n",
    "        dfs.append(df)\n",
    "    concatenated_df = pd.concat(dfs, ignore_index=True)\n",
    "    return concatenated_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Merge all the csv files of the neighborhoods sale and rent values in one DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "nei_sale_rent = concat_csvs(os.listdir(csvs_separated_by_year_path))\n",
    "nei_sale_rent.rename(columns={'Zona_Descr': 'Neighborhood', 'Descr_Tipologia': 'Type', 'Stato': 'Status', 'Compr_min': 'Min_Sale_Price',\n",
    "                     'Compr_max': 'Max_Sale_Price', 'Loc_min': 'Min_Rent_Price', 'Loc_max': 'Max_Rent_Price', 'Semestre': 'Semester', 'Anno': 'Year'}, inplace=True)\n",
    "\n",
    "# Change the types of the columns in float and int\n",
    "nei_sale_rent['Min_Rent_Price'] = nei_sale_rent['Min_Rent_Price'].astype(str).str.replace(',', '.').astype(float)\n",
    "nei_sale_rent['Max_Rent_Price'] = nei_sale_rent['Max_Rent_Price'].astype(str).str.replace(',', '.').astype(float)\n",
    "nei_sale_rent['Min_Sale_Price'] = nei_sale_rent['Min_Sale_Price'].astype(int)\n",
    "nei_sale_rent['Max_Sale_Price'] = nei_sale_rent['Max_Sale_Price'].astype(int)\n",
    "nei_sale_rent['Semester'] = nei_sale_rent['Semester'].astype(int)\n",
    "nei_sale_rent['Year'] = nei_sale_rent['Year'].astype(int)\n",
    "\n",
    "# Make neighborhoods without a comma\n",
    "nei_sale_rent['Neighborhood'] = nei_sale_rent['Neighborhood'].str.replace(',', '')\n",
    "# Check if neighborhoods have - and take the second part of the string\n",
    "nei_sale_rent['Neighborhood'] = nei_sale_rent['Neighborhood'].apply(lambda x: x.split('-')[1] if '-' in x else x)\n",
    "# Make CNA MERLATA A and C.NA MERLATA neighborhoods the same in CAMERLATA\n",
    "nei_sale_rent['Neighborhood'] = nei_sale_rent['Neighborhood'].apply(lambda x: 'CAMERLATA' if 'C.NA MERLATA' in x else x)\n",
    "nei_sale_rent['Neighborhood'] = nei_sale_rent['Neighborhood'].apply(lambda x: 'CAMERLATA' if 'CNA MERLATA' in x else x)\n",
    "nei_sale_rent['Neighborhood'] = nei_sale_rent['Neighborhood'].apply(lambda x: 'SAN CARLO' if 'SAN CARLO B.' in x else x)\n",
    "# Remove spaces from neighborhoods\n",
    "nei_sale_rent['Neighborhood'] = nei_sale_rent['Neighborhood'].str.strip()\n",
    "# Add the average sale and rent price \n",
    "nei_sale_rent['Avg_Sale_Price'] = (\n",
    "    nei_sale_rent['Min_Sale_Price'] + nei_sale_rent['Max_Sale_Price']) / 2\n",
    "nei_sale_rent['Avg_Rent_Price'] = (\n",
    "    nei_sale_rent['Min_Rent_Price'] + nei_sale_rent['Max_Rent_Price']) / 2\n",
    "\n",
    "nei_sale_rent.to_csv(os.path.join(os.getcwd(), 'rent_sale_per_neighborhood.csv'), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "import difflib\n",
    "\n",
    "def modify_array(array1, array2):\n",
    "    modified_array2 = {}\n",
    "    \n",
    "    for item2 in array2:\n",
    "        closest_match = difflib.get_close_matches(item2, array1, n=1, cutoff=0.3)\n",
    "        if closest_match:\n",
    "            modified_array2[item2] = closest_match[0]\n",
    "        else:\n",
    "            modified_array2[item2] = item2\n",
    "    \n",
    "    return modified_array2"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modify the geojson file's neighborhoods names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Neighborhoods in geojson but not in OMI files:  {'MAGENTA - S. VITTORE', 'QUINTO ROMANO', 'GORLA - PRECOTTO', 'GRECO - SEGNANO', 'LODI - CORVETTO', 'SAN SIRO', 'BANDE NERE', 'PORTA TICINESE - CONCA DEL NAVIGLIO', 'CIMIANO - ROTTOLE - Q.RE FELTRE', 'UMBRIA - MOLISE - CALVAIRATE', 'VIGENTINO - Q.RE FATIMA', 'LAMBRATE - ORTICA', \"PARCO BOSCO IN CITTA'\", \"MONLUE' - PONTE LAMBRO\", 'ROGOREDO - SANTA GIULIA', 'PTA ROMANA', 'LORETO - CASORETTO - NOLO', 'PORTA VIGENTINA - PORTA LODOVICA', 'QUARTO CAGNINO', 'DE ANGELI - MONTE ROSA', 'MACIACHINI - MAGGIOLINA', 'SCALO ROMANA', 'TRIULZO SUPERIORE', 'STAZIONE CENTRALE - PONTE SEVESO', 'STEPHENSON', 'PORTA MAGENTA', 'RONCHETTO SUL NAVIGLIO - Q.RE LODOVICO IL MORO', 'BRUZZANO', 'PARCO DELLE ABBAZIE', 'QT 8', 'VILLAPIZZONE - CAGNOLA - BOLDINASCO', 'TRENNO', 'RONCHETTO DELLE RANE', 'MORIVIONE', 'STADIO - IPPODROMI', 'QUARTO OGGIARO - VIALBA - MUSOCCO', 'FORZE ARMATE', 'PARCO FORLANINI - CAVRIANO', 'GHISOLFA', \"CITTA' STUDI\", 'MONCUCCO - SAN CRISTOFORO', 'PORTA TICINESE - CONCHETTA', 'BAGGIO - Q.RE DEGLI OLMI - Q.RE VALSESIA', 'TIBALDI', 'PARCO DEI NAVIGLI', 'PORTELLO', 'ASSIANO', 'TRE TORRI', \"NIGUARDA - CA' GRANDA - PRATO CENTENARO - Q.RE FULVIO TESTI\", 'GRATOSOGLIO - Q.RE MISSAGLIA - Q.RE TERRAZZE', 'CANTALUPA', 'ADRIANO', 'BUENOS AIRES - PORTA VENEZIA - PORTA MONFORTE', 'STADERA - CHIESA ROSSA - Q.RE TORRETTA - CONCA FALLATA', 'DERGANO', 'CASCINA MERLATA', 'ISOLA', 'PADOVA - TURRO - CRESCENZAGO', 'GIARDINI P.TA VENEZIA', 'Q.RE GALLARATESE - Q.RE SAN LEONARDO - LAMPUGNANO', 'PORTA GARIBALDI - PORTA NUOVA', 'TALIEDO - MORSENCHIO - Q.RE FORLANINI', 'MAGGIORE - MUSOCCO - CERTOSA', 'QUINTOSOLE', 'GUASTALLA'}\n",
      "['LUDOVICO IL MORO' 'P. TRENNO' 'CASTELLO' 'BOVISASCA' 'PARCO NORD'\n",
      " 'FIGINO' 'ORTOMERCATO' 'QUARTO OGGIARO' 'PIOLA' 'Q. CAGNINO' 'IPPODROMO'\n",
      " 'Q. ROMANO' 'DUOMO' 'SAN CARLO' 'COMASINA' 'ITALIA' 'TIRO SEGNO'\n",
      " 'MONTE ROSA' 'FARINI' 'ABRUZZI' 'SEMPIONE' 'CAGNOLA' 'UDINE'\n",
      " 'PARCO SEMPIONE' 'BARONA' 'STAZIONE CENTRALE VIALE STELVIO'\n",
      " 'PARCO TROTTER' 'PRECOTTO' 'FULVIO TESTI' 'ROSERIO' 'PORTA ROMANA'\n",
      " 'FORLANINI' 'PORTA TICINESE' 'TURATI' 'PAGANO' 'VIGENTINO' 'BICOCCA'\n",
      " 'LORETO' 'MUGGIANO' 'VIA SAN VITTORE' 'RONCHETTO' 'GALLARATESE'\n",
      " 'PARCO LAMBRO' 'CRESCENZAGO' 'GRATOSOGLIO' 'CERMENTATE' 'CHIARAVALLE'\n",
      " 'ARCO DELLA PACE' 'MAGGIOLINA' 'WAGNER' 'LAMBRATE' 'CHIESA ROSSA'\n",
      " 'PORTA VIGENTINA' 'XXII MARZO' 'NAVIGLIO GRANDE' 'INDIPENDENZA'\n",
      " 'PORTA NUOVA' 'AFFORI' 'BUENOS AIRES' 'SAN CRISTOFORO' 'BOVISA'\n",
      " 'GIULIO CESARE' 'GIAMBELLINO' 'CITY LIFE' 'PORTA GENOVA' 'CORSICA'\n",
      " 'CAMERLATA' 'SARPI' 'BRERA' 'SANTA GIULIA' 'TORINO' 'LORENTEGGIO']\n"
     ]
    }
   ],
   "source": [
    "# Import geojson\n",
    "neighborhood = gpd.GeoDataFrame.from_file(\n",
    "    './milan_districts.geojson')\n",
    "\n",
    "# Check which neighborhoods are in the geojson file but not in the OMI files\n",
    "neighborhoods_in_geojson = neighborhood['NIL'].unique()\n",
    "neighborhoods_in_omi = nei_sale_rent['Neighborhood'].unique()\n",
    "\n",
    "print('Neighborhoods in geojson but not in OMI files: ', set(\n",
    "    neighborhoods_in_geojson) - set(neighborhoods_in_omi))\n",
    "\n",
    "# Modify NIL column in the geojson file with the function modify_array\n",
    "neighborhood_mapping = modify_array(neighborhoods_in_omi, neighborhoods_in_geojson)\n",
    "neighborhood['NIL'] = neighborhood['NIL'].map(neighborhood_mapping)\n",
    "\n",
    "print(neighborhood['NIL'].unique())\n",
    "\n",
    "# Save the new geojson file\n",
    "neighborhood.to_file('./milan_districts_modified.geojson', driver='GeoJSON')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fix Mispellings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['id', 'listing_url', 'scrape_id', 'last_scraped', 'source', 'name',\n",
       "       'description', 'neighborhood_overview', 'picture_url', 'host_id',\n",
       "       'host_url', 'host_name', 'host_since', 'host_location', 'host_about',\n",
       "       'host_response_time', 'host_response_rate', 'host_acceptance_rate',\n",
       "       'host_is_superhost', 'host_thumbnail_url', 'host_picture_url',\n",
       "       'host_neighbourhood', 'host_listings_count',\n",
       "       'host_total_listings_count', 'host_verifications',\n",
       "       'host_has_profile_pic', 'host_identity_verified', 'neighborhood',\n",
       "       'neighborhood_cleansed', 'neighborhood_group_cleansed', 'latitude',\n",
       "       'longitude', 'property_type', 'room_type', 'accommodates', 'bathrooms',\n",
       "       'bathrooms_text', 'bedrooms', 'beds', 'amenities', 'price',\n",
       "       'minimum_nights', 'maximum_nights', 'minimum_minimum_nights',\n",
       "       'maximum_minimum_nights', 'minimum_maximum_nights',\n",
       "       'maximum_maximum_nights', 'minimum_nights_avg_ntm',\n",
       "       'maximum_nights_avg_ntm', 'calendar_updated', 'has_availability',\n",
       "       'availability_30', 'availability_60', 'availability_90',\n",
       "       'availability_365', 'calendar_last_scraped', 'number_of_reviews',\n",
       "       'number_of_reviews_ltm', 'number_of_reviews_l30d', 'first_review',\n",
       "       'last_review', 'review_scores_rating', 'review_scores_accuracy',\n",
       "       'review_scores_cleanliness', 'review_scores_checkin',\n",
       "       'review_scores_communication', 'review_scores_location',\n",
       "       'review_scores_value', 'license', 'instant_bookable',\n",
       "       'calculated_host_listings_count',\n",
       "       'calculated_host_listings_count_entire_homes',\n",
       "       'calculated_host_listings_count_private_rooms',\n",
       "       'calculated_host_listings_count_shared_rooms', 'reviews_per_month'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "airbnb = pd.read_csv('./airbnb_listings2022.csv')\n",
    "\n",
    "# Fix mispelled column names\n",
    "corrections = {\n",
    "    'neighbourhood': 'neighborhood',\n",
    "    'neighbourhood_cleansed': 'neighborhood_cleansed',\n",
    "    'neighbourhood_group_cleansed': 'neighborhood_group_cleansed',\n",
    "}\n",
    "\n",
    "airbnb.rename(columns=corrections, inplace=True)\n",
    "airbnb.columns"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create new csv with only the columns we're interested in"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/3g/5z5sh5l968zfhsm0b7025f_h0000gn/T/ipykernel_18876/421119608.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  bnb.dropna(subset=['neighborhood_cleansed'], inplace=True)\n"
     ]
    }
   ],
   "source": [
    "# Select only the columns that are useful for the analysis\n",
    "bnb = airbnb[['id', 'name', 'neighborhood_cleansed', 'latitude', 'longitude', 'host_listings_count', 'property_type', 'room_type', 'accommodates', 'bedrooms', 'price', 'availability_365', 'review_scores_rating']]\n",
    "bnb.dropna(subset=['neighborhood_cleansed'], inplace=True)\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check if the neighborhoods are the same as in the OMI file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Neighborhoods in airbnb but not in OMI files:  {'MAGENTA - S. VITTORE', 'VIALE MONZA', 'S. SIRO', 'VIGENTINA', 'NAVIGLI', 'LODI - CORVETTO', 'VILLAPIZZONE', 'SELINUNTE', 'BANDE NERE', 'BUENOS AIRES - VENEZIA', \"PARCO MONLUE' - PONTE LAMBRO\", \"NIGUARDA - CA' GRANDA\", 'RONCHETTO SUL NAVIGLIO', 'CENTRALE', 'QUARTO CAGNINO', 'EX OM - MORIVIONE', 'DE ANGELI - MONTE ROSA', 'SCALO ROMANA', 'MACIACHINI - MAGGIOLINA', 'TRIULZO SUPERIORE', 'TORTONA', 'TICINESE', 'PARCO FORLANINI - ORTICA', 'PARCO DELLE ABBAZIE', 'BRUZZANO', 'QT 8', 'PARCO AGRICOLO SUD', \"CITTA' STUDI\", 'TRENNO', 'RONCHETTO DELLE RANE', 'PARCO DEI NAVIGLI', 'FORZE ARMATE', 'UMBRIA - MOLISE', 'PORTELLO', 'GHISOLFA', 'GRATOSOGLIO - TICINELLO', 'TIBALDI', 'PARCO BOSCO IN CITT\\x85', 'TRE TORRI', 'GARIBALDI REPUBBLICA', 'CANTALUPA', 'ADRIANO', 'GIARDINI PORTA VENEZIA', 'GRECO', 'PARCO LAMBRO - CIMIANO', 'DERGANO', 'ISOLA', 'S. CRISTOFORO', 'MAGGIORE - MUSOCCO', 'STADERA', 'QUINTO ROMANO', 'QUINTOSOLE', 'GUASTALLA'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/3g/5z5sh5l968zfhsm0b7025f_h0000gn/T/ipykernel_18876/2175734261.py:9: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  bnb['neighborhood_cleansed'] = bnb['neighborhood_cleansed'].map(neighborhood_mapping)\n"
     ]
    }
   ],
   "source": [
    "neighborhoods_in_airbnb = bnb['neighborhood_cleansed'].unique()\n",
    "# Check if the neighborhoods in the airbnb file are the same as the ones in the OMI file\n",
    "print('Neighborhoods in airbnb but not in OMI files: ', set(\n",
    "    neighborhoods_in_airbnb) - set(neighborhoods_in_omi))\n",
    "\n",
    "neighborhood_mapping = modify_array(neighborhoods_in_omi, neighborhoods_in_airbnb)\n",
    "\n",
    "# Modify the neighborhoods in the airbnb file \n",
    "bnb['neighborhood_cleansed'] = bnb['neighborhood_cleansed'].map(neighborhood_mapping)\n",
    "\n",
    "# Save the file\n",
    "bnb.to_csv('./airbnb_listings2022_modified.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
